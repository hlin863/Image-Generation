{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hlin863/Image-Generation/blob/janus_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"\",\n",
    ")\n",
    "prompt = sft_format + vl_chat_processor.image_start_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "@torch.inference_mode()\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5.0,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    # Tokenize prompt\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).cuda()\n",
    "    tokens = input_ids.repeat(parallel_size * 2, 1)\n",
    "\n",
    "    # Efficient padding for unconditional samples\n",
    "    tokens[1::2, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    # Get input embeddings\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    # Prepare storage for generated tokens\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    # Initialize past_key_values\n",
    "    past_key_values = None\n",
    "\n",
    "    # Generate tokens iteratively\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state[:, -1, :].unsqueeze(1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Compute logits\n",
    "        logits = mmgpt.gen_head(hidden_states.squeeze(1))\n",
    "        logit_cond, logit_uncond = logits[0::2], logits[1::2]\n",
    "\n",
    "        # Conditional Free Guidance (CFG)\n",
    "        cfg_scale = torch.sigmoid(torch.tensor(cfg_weight))\n",
    "        logits = logit_uncond + cfg_scale * (logit_cond - logit_uncond)\n",
    "\n",
    "        # Stable softmax with temperature scaling\n",
    "        logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        # Efficient sampling\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "        generated_tokens[:, i] = next_token\n",
    "\n",
    "        # Prepare next input embeddings\n",
    "        next_token_expanded = torch.cat([next_token.unsqueeze(1), next_token.unsqueeze(1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token_expanded)\n",
    "        inputs_embeds = img_embeds.unsqueeze(1)\n",
    "\n",
    "    # Decode generated images\n",
    "    dec = mmgpt.gen_vision_model.decode_code(\n",
    "        generated_tokens.to(dtype=torch.int), \n",
    "        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size]\n",
    "    )\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    # Post-processing and clipping\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Save generated images efficiently\n",
    "    os.makedirs('generated_samples', exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join('generated_samples', f\"img_{i}.jpg\")\n",
    "        Image.fromarray(dec[i]).save(save_path)\n",
    "\n",
    "    print(f\"Generated {parallel_size} images successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    vl_gpt,\n",
    "    vl_chat_processor,\n",
    "    prompt,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
